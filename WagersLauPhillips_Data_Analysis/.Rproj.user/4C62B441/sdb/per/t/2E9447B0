{
    "contents" : "---\ntitle: \"Wagers, Lau & Phillips (2009) Analysis Transcript\"\noutput: \n  html_document:\n    toc: true\n    theme: united\n---\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. You can access the source HERE.\n\n```{r prepareWorkSpace, cache=FALSE}\n# Load all data\nload(\"WagersLauPhillips.rawdata.Rdata\")\n\n# Libraries\nlibrary(magrittr)\nlibrary(lme4)\nlibrary(gplots)\nlibrary(knitr)\n\n# Analysis wide options and helper functions\noptions(digits=3)\n\nstderr <- function(x){ sd(x, na.rm=TRUE) / sqrt(length(x[!is.na(x)])) }\n\nshaveHalfPercentile <- function(df, verbose=FALSE){\n  RT_bounds <- quantile(df$RT, c(0.005, 0.995))\n  shaved <- subset(df, RT >= RT_bounds[1] & RT < RT_bounds[2])\n  range(df$RT) %>%\n    paste(collapse=\"-\") %>%\n    paste(\"Original range (ms):\", .) %>%\n    message\n  \n  range(shaved$RT) %>%\n    paste(collapse=\"-\") %>%\n    paste(\"Shaved range (ms):\", .) %>%\n    message\n  \n  return(shaved)\n}\n\nexcludeIncorrectTrials <- function(df){\n  correct_only <- subset(df, QPCT==100)\n  return(correct_only)\n}\n\nunstressModel <- function(fm){\n  resid(fm) %>%\n    scale %>%\n    abs -> scaled_residuals\n  \n  keep.these.obs <- which(scaled_residuals < 3)\n  stressors <- which(scaled_residuals >=3)\n  return(list(keep.these.obs, stressors))\n}\n\npairPlot <- function(means.table, se.table, range.x, range.y=c(-999,-999), \n                     par.pch=20, legend.labels){\n  if(range.y[1]==-999){\n    range.y <- range(means.table)\n    range.y <- 50*(range.y%/%50 + c(-1,1)) \n  }\n  \n  plotCI(range.x, means.table[1, range.x],\n         uiw = se.table[1, range.x], gap=0, ylim=range.y,\n         type='b', pch=par.pch, pt.bg=\"black\", cex=1.5, \n         bty='n', ylab=\"RT (ms)\", xlab=\"Region\")\n  \n#  grid()\n\n  plotCI(range.x, means.table[2, range.x], add=1,\n         uiw = se.table[2, range.x], gap=0,\n         type='b', pch=par.pch, pt.bg=\"white\", cex=1.5)\n  \n  if(!is.null(legend.labels)){\n      legend(x=\"topright\", legend = legend.labels, fill=c(\"black\",\"white\"), bty='n')\n  }\n}\n```\n\n# Experiment 1\n\n```\nThe old key/s unsurprisingly was/were rusty from many years of disuse\nThe old key/s unsurprisingly were/was rusty from many years of disuse\n```\n\n```{r exp1::prepareData, cache=TRUE, echo=FALSE}\n\nrelabelExperiment1 <- function(df){\n  df$SubjectNumber <- df$Condition\n  levels(df$SubjectNumber) <- c(rep(\"sg\", 2),\n                                rep(\"pl\", 2))\n  contrasts(df$SubjectNumber) <- -contr.sum(2)/2\n\n  df$Grammaticality <- df$Condition\n  levels(df$Grammaticality) <- c(\"gr\", \"un\", \"un\", \"gr\")\n  contrasts(df$Grammaticality) <- -contr.sum(2)/2\n  \n  df$Correct <- df$QPCT/100\n\n  return(df)\n}\n\nrelabelExperiment1(Experiment1) -> Experiment1\nshaveHalfPercentile(Experiment1, verbose=TRUE) %>%\n  excludeIncorrectTrials -> exp1.target\n```\n\n### Exp. 1 :: Accuracy\n\n```{r exp1:summarizeAccuracy, echo=FALSE}\nExperiment1 %$%\n  tapply(Correct, list(Grammaticality, SubjectNumber), mean) ->\n  exp1_accuracy.table\n\nkable(100*exp1_accuracy.table, caption=\"Experiment 1 Accuracy by Condition\", format=\"pandoc\")\n```\n\nThere *is* a small effect of Grammaticality.\n\n```{r exp1:modelAccuracy, echo=FALSE}\n\n\nexp1_acc.fm0 <- glmer(Correct ~ Grammaticality*SubjectNumber + (1|Subj) + (1|Item), \n      data = subset(Experiment1, Region==1), family=binomial)\nsummary(exp1_acc.fm0)\n```\n\n```{r exp1::summarizeWordByWord, echo=FALSE}\nexp1.target %$%\n  tapply(RT, list(Grammaticality, Region, SubjectNumber), mean) -> \n  region_means.table\n\n# Use participant-wise standard errors\nexp1.target %$%\n  tapply(RT, list(Grammaticality, Region, SubjectNumber, Subj), mean) -> regionxsubj_means.table\n\napply(regionxsubj_means.table, c(1,2,3), stderr) -> region_se.table\n```\n\n### Exp. 1 :: Reading Times\n\n```{r exp1::printRT_Tables, echo=FALSE}\nrange.x <- 1:14\nkable(region_means.table[,range.x,'sg'], \n      caption=\"Singular subjects; (ms)\")\nkable(region_means.table[,range.x,'pl'], \n      caption=\"Plural subjects; (ms)\")\n```\n\n```{r exp1::visualizeWordByWord, echo=FALSE}\npar(mfrow=c(2,1), mar=c(4,4,3,0))\npairPlot(region_means.table[,,'sg'], region_se.table[,,'sg'], \n         range.x = 1:14, range.y = c(250,600), par.pch = 21,\n         legend.labels = c(\"grammatical\", \"ungrammatical\"))\ntitle(main=\"Singular subjects\")\npairPlot(region_means.table[,,'pl'], region_se.table[,,'pl'], \n         range.x = 1:14, range.y = c(250,600), par.pch = 22,\n         legend.labels = NULL)\ntitle(main=\"Plural subjects\")\n\nmessage(\"Plotted standard errors calculated over per-participant means\")\n\n```\n\n## Experiment1 Verb\n\n**Claim**: Ungrammatical sentences take longer to read on the verb.\n\n```{r exp1:modelWordRegion_verb, cache=TRUE}\nwoi <- 5\ndoi <- subset(exp1.target, Region==woi)\n\nexp1.formula <- as.formula(\"RT ~ SubjectNumber*Grammaticality + (SubjectNumber*Grammaticality|Subj) + (SubjectNumber*Grammaticality|Item)\")\n\nverb.fm0 <- lmer(exp1.formula, data=doi)\nverb.fm1 <- update(verb.fm0, \n                   data=doi[unstressModel(verb.fm0)[[1]], ])\n\nsummary(verb.fm0)\nsummary(verb.fm1)\nwith(doi[unstressModel(verb.fm0)[[1]], ],\n     interaction.plot(SubjectNumber, Grammaticality, RT),\n     ylim=c(400,525))\n\n#par(mfcol=c(1,1), mar=c(4,4,3,3))\n#qqnorm(resid(verb.fm0))\n#qqnorm(resid(verb.fm1))\n```\n\n\n## Experiment1 Adverb\n\nClaim (@): Plurals take longer to process, reflected by an effect of complexity on the following region (adverb).\n\n```{r exp1:modelWordRegion_adv, cache=TRUE}\n# Convergence problem with fully-saturated model;\n# This steps off Num*Gram interaction within-subject\n# To Num+Gram within-subject\n\nexp1.formula.sA_B <- as.formula(\"RT ~ SubjectNumber*Grammaticality + (SubjectNumber+Grammaticality|Subj) + (SubjectNumber*Grammaticality|Item)\")\n\nwoi <- 4\ndoi <- subset(exp1.target, Region==woi)\nadverb.fm0 <- lmer(exp1.formula.sA_B, data=doi)\nadverb.fm1 <- update(adverb.fm0,\n                     data=doi[unstressModel(adverb.fm0)[[1]], ])\nsummary(adverb.fm0)\nsummary(adverb.fm1)\n\nwith(doi[unstressModel(adverb.fm0)[[1]], ],\n     interaction.plot(Grammaticality, SubjectNumber, RT),\n     ylim=c(400,525))\n```\n\n",
    "created" : 1447681180358.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2389390469",
    "id" : "2E9447B0",
    "lastKnownWriteTime" : 1447691568,
    "path" : "~/Dropbox/Desktop.Fanihi/WagersLauPhillips_DataRepository/WagersLauPhillips.AnalysisTranscript.Rmd",
    "project_path" : "WagersLauPhillips.AnalysisTranscript.Rmd",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}